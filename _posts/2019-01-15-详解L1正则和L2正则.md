---
layout:     post
title:      详解L1正则和L2正则
subtitle:   详解L1正则和L2正则
date:       2019-1-15
author:     fzr
header-img: 
catalog: true
tags:
    - machine learning
---
# 详解L1正则和L2正则

之前看过很多L1正则和L2正则分析的文章，也看了很多比较二者区别的文章，但始终没能总结成体系，写这篇文章的目的就是想总结一下关于L1正则和L2正则的分析。

正则化方法 (Regularization) 是机器学习领域中一种非常重要的技巧，它主要用来对权重系数加以约束限制，进而防止过拟合。数学上来讲，正则化即为在目标函数中加入对权值系数的约束。



首先说说使用正则化的场景：

1. 变量较多或者样本量少于变量数的情形；

2. 特征集合具有比较高的共线性；

3. 寻求稀疏解（比如估计模型参数时的嵌入式特征选择）；

4. 为了解释或寻求高维数据集中变量组合；

正则化的作用在于：

1）保证模型尽可能的简单，避免过拟合。

2）约束模型特性，加入一些先验知识，例如稀疏、低秩等。

说到正则化的作用，先要搞清楚两个问题

1）实现参数的稀疏有什么好处吗？

一个好处是可以简化模型，避免过拟合。因为一个模型中真正重要的参数可能并不多，如果考虑所有的参数起作用，那么可以对训练数据可以预测的很好，但是对测试数据就只能呵呵了。另一个好处是参数变少可以使整个模型获得更好的可解释性。

2）参数值越小代表模型越简单吗？

是的。为什么参数越小，说明模型越简单呢，这是因为越复杂的模型，越是会尝试对所有的样本进行拟合，甚至包括一些异常样本点，这就容易造成在较小的区间里预测值产生较大的波动，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数。因此复杂的模型，其参数值会比较大。



了解这些前提知识以后，可以看L1和L2的区别：

L1正则化与L2正则化形式上的区别在于范数的阶。这两种正则化的主要区别在于以下几点：

解的唯一性：L2正则化具有唯一解，而L1正则化没有唯一解。

![img/2019-01-15/L1与L2正则.png][]


图一：本图阐释了L1正则化和L2正则化解的唯一性，其中绿色的是L2正则化约束，其余三条是L1正则化约束。

内嵌特征选择：L1具有内嵌的特征选择特性，而L2没有。L1正则化会得到稀疏的系数解，即少数权重系数非零，多数权重系数经过L1正则约束值为零。L2正则约束不会产生稀疏的系数解，因此不具特征选择特性。

稀疏性：即只有少数权重系数非零，L1正则化具有稀疏性，它会使得多数权重系数为零，或者少数比较大的权重系数。

计算复杂度：L1正则化没有解析解，而L2正则化具有解析解。这使得L2正则化计算复杂度更低。对于非稀疏的情形，L1正则化计算复杂度较高。



这是从应用场景上区分L1和L2.但这些解释还是趋于表面说法了

接下来我们要从原理上区分L1和L2，同时也能理解L1能稀疏化参数求解结果，L2能防止过拟合的根本原因了

主要从三个角度来理解L1正则化的原理：从优化问题的角度，从梯度的角度和从概率的角度



L1为什么可以实现稀疏化？L1利用的是绝对值函数的尖峰



优化问题的角度：

![img/2019-01-15/L11.png][]

从梯度的角度来看：

![img/2019-01-15/L12.png][]



从概率的角度来看：

![img/2019-01-15/L13.png][]




